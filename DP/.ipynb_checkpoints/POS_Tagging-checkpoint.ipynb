{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c943646f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9616432b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vocabulary generate kore pura training corpus er\n",
    "\n",
    "def generate_vocab(json_file_path, threshold=2):\n",
    "    with open(json_file_path, \"r\") as json_file:\n",
    "        json_data = json.load(json_file)\n",
    "\n",
    "    vocab = {'<unk>': 0}\n",
    "    counter = 0\n",
    "\n",
    "    # Counting the number of times each word occurs\n",
    "    for entry in json_data:\n",
    "        sentence = entry[\"sentence\"]\n",
    "        counter+=1\n",
    "\n",
    "        for word in sentence:\n",
    "            for subword in word.split():\n",
    "                if subword.strip():\n",
    "                    if subword not in vocab:\n",
    "                        vocab[subword] = 1\n",
    "                    else:\n",
    "                        vocab[subword] += 1\n",
    "\n",
    "    # Adding count of rare words to the count of <unk>\n",
    "    for k in list(vocab.keys())[1:]:\n",
    "        if vocab[k] < threshold:\n",
    "            vocab['<unk>'] += vocab[k]\n",
    "            del vocab[k]\n",
    "\n",
    "    # Sort vocab by count in descending order\n",
    "    sorted_vocab = {k: v for k, v in sorted(vocab.items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "    # Push <unk> to the top\n",
    "    unk_val = sorted_vocab.pop('<unk>')\n",
    "    sorted_vocab = {'<unk>': unk_val, **sorted_vocab}\n",
    "\n",
    "    with open('vocab.txt', 'w') as f:\n",
    "        format_str = ''\n",
    "        for index, (key, count) in enumerate(sorted_vocab.items()):\n",
    "            format_str += key + '\\t' + str(index) + '\\t' + str(count) + '\\n'\n",
    "        f.write(format_str)\n",
    "\n",
    "    #print(\"Threshold =\", threshold)\n",
    "\n",
    "    return sorted_vocab, counter\n",
    "\n",
    "vocab, counter = generate_vocab(\"C:/Users/Admin/Desktop/Thesis/POS Tagging/archive/train2.json\")\n",
    "print(\"Overall size of my vocabulary =\", len(vocab))\n",
    "print(\"No. of times '<unk>' occurs in my vocabulary =\", vocab['<unk>'])\n",
    "print(\"No. of sentence =\", counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ddf6977",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_training_data(file_path):\n",
    "    with open(file_path, 'r') as json_file:\n",
    "        return json.load(json_file)\n",
    "\n",
    "#transition holo NNP to VBP kotobar, emission holo \"Book\" word ta kotobar NNP r kotobar VBP\n",
    "#unique state holo distinct tag gulo niye banano list\n",
    "def process_training_data(data, vocab):\n",
    "    transition_probabilities = {}\n",
    "    emission_probabilities = {}\n",
    "    unique_states = []\n",
    "\n",
    "    for entry in data:\n",
    "        sentence_tokens = entry['sentence']\n",
    "        labels = entry['labels']\n",
    "        prior_state = 'head'  \n",
    "\n",
    "        for word, tag in zip(sentence_tokens, labels):\n",
    "            if word not in vocab.keys():\n",
    "                word = '<unk>'\n",
    "\n",
    "            # Update unique states\n",
    "            if tag not in unique_states:\n",
    "                unique_states.append(tag)\n",
    "\n",
    "            # Update transition count\n",
    "            transition_key = (prior_state, tag)\n",
    "            if transition_key not in transition_probabilities:\n",
    "                transition_probabilities[transition_key] = 1\n",
    "            else:\n",
    "                transition_probabilities[transition_key] += 1\n",
    "\n",
    "            # Update emission count\n",
    "            emission_key = (tag, word)\n",
    "            if emission_key not in emission_probabilities:\n",
    "                emission_probabilities[emission_key] = 1\n",
    "            else:\n",
    "                emission_probabilities[emission_key] += 1\n",
    "\n",
    "            prior_state = tag\n",
    "\n",
    "    return transition_probabilities, emission_probabilities, unique_states\n",
    "\n",
    "def normalize_probabilities(probabilities):\n",
    "    normalized_probabilities = {}\n",
    "    for key in probabilities:\n",
    "        state, value = key\n",
    "        total = sum(v for k, v in probabilities.items() if k[0] == state)\n",
    "        normalized_probabilities[key] = probabilities[key] / total\n",
    "    return normalized_probabilities\n",
    "\n",
    "#transition r emission gulo k dictionary er moto kore dekhano hoyese\n",
    "def save_hmm_model(transition_probs, emission_probs, output_file):\n",
    "    \n",
    "    # Convert tuple keys to strings\n",
    "    transition_probs = {str(k): v for k, v in transition_probs.items()}\n",
    "    emission_probs = {str(k): v for k, v in emission_probs.items()}\n",
    "\n",
    "    hmm_model = {\n",
    "        'transition': transition_probs,\n",
    "        'emission': emission_probs,\n",
    "    }\n",
    "\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(hmm_model, f)\n",
    "\n",
    "    return transition_probs, emission_probs\n",
    "\n",
    "training_data = load_training_data('C:/Users/Admin/Desktop/Thesis/POS Tagging/archive/train2.json')\n",
    "transition_probs, emission_probs, unique_states = process_training_data(training_data, vocab)\n",
    "normalized_transition_probs = normalize_probabilities(transition_probs)\n",
    "normalized_emission_probs = normalize_probabilities(emission_probs)\n",
    "transition_param_dict, emission_param_dict = save_hmm_model(normalized_transition_probs, normalized_emission_probs, 'hmm.json')\n",
    "\n",
    "print(\"No. of transition parameters =\", len(transition_param_dict))\n",
    "print(\"No. of emission parameters =\", len(emission_param_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e77ba2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc99abdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_text = input(\"Enter your sentence: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c305a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(user_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34341b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test file theke data read kore r format kore\n",
    "\n",
    "def format_data(user_text):\n",
    "    text = user_text\n",
    "    sentence_tokens = nltk.word_tokenize(text)\n",
    "    #print(text)\n",
    "    return sentence_tokens\n",
    "\n",
    "#POS tag kore each word er r accuracy dey\n",
    "def greedy_decoding(formatted_data, vocab, transition_param_dict, emission_param_dict, state_track):\n",
    "    accuracy = []\n",
    "\n",
    "    prior_st = 'head'\n",
    "    for word in formatted_data:\n",
    "        if word not in vocab.keys():\n",
    "            word = '<unk>'\n",
    "\n",
    "        max_prob = 0\n",
    "        max_st = None\n",
    "\n",
    "        for st_ in state_track:\n",
    "            tr = transition_param_dict.get(str((prior_st, st_)), 1e-7)\n",
    "            em = emission_param_dict.get(str((st_, word)), 0)\n",
    "            prob = tr * em\n",
    "\n",
    "            if prob > max_prob:\n",
    "                max_prob = prob\n",
    "                max_st = st_\n",
    "\n",
    "        #accuracy.append(st == max_st)\n",
    "        '''prior_st = max_st\n",
    "        print(prior_st)'''\n",
    "        \n",
    "        print(max_st)\n",
    "        print('Transition_prob = ',transition_param_dict.get(str((prior_st, max_st)), 1e-7))\n",
    "        print('Emission_prob = ', emission_param_dict.get(str((max_st, word)), 0))\n",
    "        prior_st = max_st\n",
    "\n",
    "    return #sum(accuracy) / len(accuracy)\n",
    "\n",
    "formatted_dev_data = format_data(user_text)\n",
    "print(formatted_dev_data)\n",
    "greedy_decoding(formatted_dev_data, vocab, transition_param_dict, emission_param_dict, unique_states)\n",
    "#print(emission_param_dict.get(str(('VB', 'book')), 0))\n",
    "#accuracy = greedy_decoding(formatted_dev_data, vocab, transition_param_dict, emission_param_dict, unique_states)\n",
    "#print(\"Accuracy on dev data with greedy decoding =\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b8a1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "## eita nibo\n",
    "def reverse_graph(G):\n",
    "    '''Return the reversed graph g[dst][src]=G[src][dst]'''\n",
    "    g = {}\n",
    "    for src in G.keys():\n",
    "        for dst in G[src].keys():\n",
    "            if dst not in g.keys():\n",
    "                g[dst] = {}\n",
    "            g[dst][src] = G[src][dst]\n",
    "    return g\n",
    "\n",
    "\n",
    "def build_max(rg, root):\n",
    "    '''Find the max in-edge for every node except for the root.'''\n",
    "    mg = {}\n",
    "    for dst in rg.keys():\n",
    "        if dst == root:\n",
    "            continue\n",
    "        max_ind = -100\n",
    "        max_value = -100\n",
    "        for src in rg[dst].keys():\n",
    "            if rg[dst][src] >= max_value:\n",
    "                max_ind = src\n",
    "                max_value = rg[dst][src]\n",
    "        mg[dst] = {max_ind: max_value}\n",
    "    return mg\n",
    "\n",
    "\n",
    "def find_circle(mg):\n",
    "    '''Return the first circle if find, otherwise return None'''\n",
    "\n",
    "    for start in mg.keys():\n",
    "        visited = []\n",
    "        stack = [start]\n",
    "        while stack:\n",
    "            n = stack.pop()\n",
    "            if n in visited:\n",
    "                C = []\n",
    "                while n not in C:\n",
    "                    C.append(n)\n",
    "                    n = list(mg[n].keys())[0]\n",
    "                return C\n",
    "            visited.append(n)\n",
    "            if n in mg.keys():\n",
    "                stack.extend(list(mg[n].keys()))\n",
    "    return None\n",
    "\n",
    "\n",
    "def chu_liu_edmond(G, root):\n",
    "    ''' G: dict of dict of weights\n",
    "            G[i][j] = w means the edge from node i to node j has weight w.\n",
    "        root: the root node, has outgoing edges only.\n",
    "    '''\n",
    "    # reversed graph rg[dst][src] = G[src][dst]\n",
    "    rg = reverse_graph(G)\n",
    "    # root er only out edge\n",
    "    rg[root] = {}\n",
    "    # the maximum edge select korlam for each node other than root\n",
    "    mg = build_max(rg, root)\n",
    "\n",
    "    # check if mg is a tree (contains a circle)\n",
    "    C = find_circle(mg)\n",
    "    # if there is no circle, it means mg is what we want\n",
    "    if not C:\n",
    "        return reverse_graph(mg)\n",
    "\n",
    "    # jesob node circle kore tader k niye compact node korlm\n",
    "    all_nodes = G.keys()\n",
    "    vc = max(all_nodes) + 1\n",
    "\n",
    "    # new graph holo G_prime\n",
    "    V_prime = list(set(all_nodes) - set(C)) + [vc]\n",
    "    G_prime = {}\n",
    "    vc_in_idx = {}\n",
    "    vc_out_idx = {}\n",
    "    # Now add the edges to G_prime\n",
    "    for u in all_nodes:\n",
    "        for v in G[u].keys():\n",
    "            # incoming edge er weight calculation\n",
    "            if (u not in C) and (v in C):\n",
    "                if u not in G_prime.keys():\n",
    "                    G_prime[u] = {}\n",
    "                w = G[u][v] - list(mg[v].values())[0]\n",
    "                if (vc not in G_prime[u]) or (vc in G_prime[u] and w > G_prime[u][vc]):\n",
    "                    G_prime[u][vc] = w\n",
    "                    vc_in_idx[u] = v\n",
    "\n",
    "            # outgoing edge er weight calculation\n",
    "            elif (u in C) and (v not in C):\n",
    "                if vc not in G_prime.keys():\n",
    "                    G_prime[vc] = {}\n",
    "                w = G[u][v]\n",
    "                if (v not in G_prime[vc]) or (v in G_prime[vc] and w > G_prime[vc][v]):\n",
    "                    G_prime[vc][v] = w\n",
    "                    vc_out_idx[v] = u\n",
    "\n",
    "            # Third case: if the source and dest are all not in the circle, then just add the edge to the new graph.\n",
    "            elif (u not in C) and (v not in C):\n",
    "                if u not in G_prime.keys():\n",
    "                    G_prime[u] = {}\n",
    "                G_prime[u][v] = G[u][v]\n",
    "\n",
    "    # Recursively run the algorihtm on the new graph G_prime\n",
    "    A = chu_liu_edmond(G_prime, root)\n",
    "    # print(A)\n",
    "\n",
    "    # compacted node k vangbo, erpor incoming r outgoing edge gulo identify krbo\n",
    "    # always max ta choose krbo r bakigulo delete krbo\n",
    "    all_nodes_A = list(A.keys())\n",
    "    for src in all_nodes_A:\n",
    "        # The number of out-edges varies, could be 0 or any number <=|V\\C|\n",
    "        if src == vc:\n",
    "            for node_in in A[src].keys():\n",
    "                orig_out = vc_out_idx[node_in]\n",
    "                if orig_out not in A.keys():\n",
    "                    A[orig_out] = {}\n",
    "                A[orig_out][node_in] = G[orig_out][node_in]\n",
    "        else:\n",
    "            #for dst in A[src]:\n",
    "            for dst in list(A[src].keys()):\n",
    "                # There must be only one in-edge to vc.\n",
    "                if dst == vc:\n",
    "                    orig_in = vc_in_idx[src]\n",
    "                    A[src][orig_in] = G[src][orig_in]\n",
    "                    del A[src][dst]\n",
    "    del A[vc]\n",
    "\n",
    "\n",
    "    for node in C:\n",
    "        if node != orig_in:\n",
    "            src = list(mg[node].keys())[0]\n",
    "            if src not in A.keys():\n",
    "                A[src] = {}\n",
    "            A[src][node] = mg[node][src]\n",
    "\n",
    "    return A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f681ce42",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = {\n",
    "    0: {1: 9, 2: 10, 3: 9},  # Vertex 0 has edges to vertices 1, 2 and 3 with weights 9, 10, 9\n",
    "    1: {2: 20, 3: 3},\n",
    "    2: {1: 30, 3: 30},\n",
    "    3: {1: 11, 2: 0}\n",
    "}\n",
    "print(chu_liu_edmond(G, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fcf7a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(formatted_dev_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4280ed3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(transition_param_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7291f3b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(emission_param_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b1fe13",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(unique_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b34f51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec549c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
